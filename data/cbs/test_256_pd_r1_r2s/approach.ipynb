{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Rundown of analysis, training, and prediction of multichannel segmentation\n",
    "\n",
    "The whole process consists of several steps that need to be repeated when training networks for different contrasts or resolutions.\n",
    "Here, we try to train a dedicated network for the AD/PCA highres data.\n",
    "The input for the segmentation will be\n",
    "\n",
    "- 500 um resolution scans of the whole head\n",
    "- we train on multichannel data that consists of PD, R1, and R2s maps\n",
    "- the plain input maps will be clipped and rescaled to suitable value ranges (because noisy regions contain large outliers that would reduce the range of meaningful contrast for the network to learn from)\n",
    "\n",
    "To run the whole pipeline, the following steps were necessary:\n",
    "\n",
    "1. Decide how many different contrasts you want as input for your network. Remember that for this approach the contrasts need to be perfectly co-registered. At the moment, I believe this makes the most sense when you're using multiple echoes or, like here, quantitative maps that show different features.\n",
    "2. Inspect the images and find suitable value ranges for all contrasts. These ranges are then used to clip and rescale the maps for further processing.\n",
    "3. For at least one set of input images, you need a segmentation. It doesn't need to be a perfect segmentation, but it will be used to calculate the regional gray value statistics for each segmentation class.\n",
    "4. From this analysis, a dedicated _generator configuration_ is created for you. It contains the specification for the random distribution for every segmentation class. This specification is used to create random synthetic maps that should resemble your original data as close as possible. In the case here, this will not be perfect because maps like R2s have gray value distributions in some regions that can not really be modeled with a Gaussian.\n",
    "5. The generator configuration is used to create the training data for the neural network. There are two options: You can create the training data on-the-fly, or you can create them beforehand and store them in a TensorFlow format that is easy to deserialize. The latter approach has another advantage. You can train different network parametrizations with the same training data and compare the performance.\n",
    "6. With the training data (or the generation configuration), you derive a training configuration that further specifies the used network model and its hyperparameters.\n",
    "7. During the training, we write out the weights and biases of the model as snapshots. These can be loaded back if you want to create a network for prediction (aka running the actual segmentation)\n",
    "\n",
    "## 1. & 2. Used Contrasts and Value Ranges\n",
    "\n",
    "## 2. & 3. Running the Region Analysis and Create the Generator Config\n",
    "\n",
    "## 4. & 5. Creating the Training Data\n",
    "\n",
    "## 6. Training Configuration and Training on HPC\n",
    "\n",
    "# Importing necessary libraries and modules\n"
   ],
   "id": "779dbc0ec8e75538"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T12:27:16.212765Z",
     "start_time": "2024-01-11T12:27:14.171100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 13:27:14.206589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 13:27:14.279029: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-11 13:27:15.595196: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 13:27:15.639732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-11 13:27:15.639776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: amos\n",
      "2024-01-11 13:27:15.639787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: amos\n",
      "2024-01-11 13:27:15.639944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.147.5\n",
      "2024-01-11 13:27:15.639988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.147.5\n",
      "2024-01-11 13:27:15.639998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.147.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "import commands.cbs_predict as pred\n",
    "from SynthSeg.training_options import TrainingOptions\n",
    "from SynthSeg.brain_generator_options import GeneratorOptions\n",
    "\n",
    "\n",
    "training_config_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/training.yml\"\n",
    "generator_config_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/generator.yml\"\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "# Continue with the prediction part of your code.\n",
    "\n",
    "training_options = TrainingOptions.load_yaml(training_config_file)\n",
    "generator_options = GeneratorOptions.load_yaml(generator_config_file)\n",
    "\n",
    "training_options = training_options.convert_lists_to_numpy()\n",
    "generator_options = generator_options.convert_lists_to_numpy()\n",
    "\n",
    "predict_options = pred.PredictOptions(\n",
    "    training_config=training_config_file,\n",
    "    path_images=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/tfrecord2nifty/images\",\n",
    "    gt_folder=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/tfrecord2nifty/labels\",\n",
    "    path_model=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/dice_063.h5\",\n",
    "    path_segmentations=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/segmentations/\",\n",
    "    path_posteriors=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/posteriors/\",\n",
    "    target_res=training_options.target_res,\n",
    ")"
   ],
   "id": "6911d9f6c7020e53",
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T12:50:20.016090Z",
     "start_time": "2024-01-11T12:37:02.322328Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting 1/8\n",
      "1/1 [==============================] - 17s 17s/step\n",
      "predicting 2/8    remaining time: 0:11:09\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 3/8    remaining time: 0:09:16\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "predicting 4/8    remaining time: 0:07:45\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "predicting 5/8    remaining time: 0:06:11\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 6/8    remaining time: 0:04:41\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 7/8    remaining time: 0:03:07\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 8/8    remaining time: 0:01:33\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "evaluating 1/8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred.predict(\n",
    "    predict_options.path_images,\n",
    "    predict_options.path_segmentations,\n",
    "    predict_options.path_model,\n",
    "    generator_options.output_labels,\n",
    "    n_neutral_labels=generator_options.n_neutral_labels,\n",
    "    names_segmentation=predict_options.names_segmentation,\n",
    "    path_posteriors=predict_options.path_posteriors,\n",
    "    path_resampled=predict_options.path_resampled,\n",
    "    path_volumes=predict_options.path_volumes,\n",
    "    min_pad=predict_options.min_pad,\n",
    "    cropping=predict_options.cropping,\n",
    "    target_res=predict_options.target_res,\n",
    "    gradients=predict_options.gradients,\n",
    "    flip=predict_options.flip,\n",
    "    topology_classes=predict_options.topology_classes,\n",
    "    sigma_smoothing=predict_options.sigma_smoothing,\n",
    "    keep_biggest_component=predict_options.keep_biggest_component,\n",
    "    n_levels=training_options.n_levels,\n",
    "    nb_conv_per_level=training_options.nb_conv_per_level,\n",
    "    conv_size=training_options.conv_size,\n",
    "    unet_feat_count=training_options.unet_feat_count,\n",
    "    feat_multiplier=training_options.feat_multiplier,\n",
    "    activation=training_options.activation,\n",
    "    gt_folder=predict_options.gt_folder,\n",
    "    evaluation_labels=predict_options.evaluation_labels,\n",
    "    list_incorrect_labels=predict_options.list_incorrect_labels,\n",
    "    list_correct_labels=predict_options.list_correct_labels,\n",
    "    compute_distances=predict_options.compute_distances,\n",
    "    recompute=predict_options.recompute,\n",
    "    verbose=predict_options.verbose,\n",
    "    use_original_unet=training_options.use_original_unet\n",
    ")\n"
   ],
   "id": "e9ec5c56cf270cec",
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T03:56:08.083859Z",
     "start_time": "2024-01-11T03:56:08.078178Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,  15,  16,  24,  72,  85, 502, 506, 507, 508, 509, 511,\n",
       "       512, 514, 515, 516, 530,   2,   3,   4,   5,   7,   8,  10,  11,\n",
       "        12,  13,  17,  18,  25,  26,  28,  30, 136, 137,  41,  42,  43,\n",
       "        44,  46,  47,  49,  50,  51,  52,  53,  54,  57,  58,  60,  62,\n",
       "       163, 164])"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "generator_options.generation_labels"
   ],
   "id": "771294538c272ade",
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "import SynthSeg.segmentation_model as M\n",
    "import commands.tfrecord_to_nifti as tfnii\n",
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "# Uncomment this if you run out of GPU memory when predicting\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "print(\"Using GPU\" if tf.test.gpu_device_name() else \"Using CPU\")\n",
    "\n",
    "tfrecord_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/017132.tfrecord\"\n",
    "nifty_dir = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/tfrecord2nifty/\"\n",
    "net_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/dice_063.h5\"\n",
    "output_dir = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/cbs_data/\"\n",
    "# output_dir = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/training_data/\"\n",
    "\n",
    "\n",
    "def adjust_size(input_data: np.ndarray, desired_shape: List[int]):\n",
    "    \"\"\"\n",
    "    Adjusts the size of the input data array to match the desired shape.\n",
    "\n",
    "    It does it by either cropping or padding symmetrically.\n",
    "\n",
    "    Args:\n",
    "        input_data: The input data array.\n",
    "        desired_shape: The desired shape for the input data array.\n",
    "\n",
    "    Returns:\n",
    "        The input data array adjusted to the desired shape.\n",
    "    \"\"\"\n",
    "    current_shape = input_data.shape\n",
    "    adjustments = [(ds - cs) / 2. for ds, cs in zip(desired_shape, current_shape)]\n",
    "    pad_values = [(int(np.floor(adj)), int(np.ceil(adj))) if adj > 0 else (0, 0) for adj in adjustments]\n",
    "    crop_values = [(int(np.floor(-adj)), -int(np.ceil(-adj))) if adj < 0 else (0, 0) for adj in adjustments]\n",
    "    content_values = [(int(np.floor(adj)), int(cs + np.ceil(adj))) for adj, cs in zip(adjustments, current_shape)]\n",
    "    # Pad / Crop the array for non-negative / negative adjustments (respectively)\n",
    "    input_data_adjusted = np.pad(input_data, pad_width=pad_values, mode='constant', constant_values=0)\n",
    "    input_data_adjusted = input_data_adjusted[\n",
    "                          crop_values[0][0]:content_values[0][1],\n",
    "                          crop_values[1][0]:content_values[1][1],\n",
    "                          crop_values[2][0]:content_values[2][1]]\n",
    "\n",
    "    return input_data_adjusted\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "# Create nifti from a tfrecord file.\n",
    "# tfnii.tfrecord_to_nifty(tfnii.Options(input_file=tfrecord_file, output_directory=nifty_dir))\n",
    "\n",
    "print(\"Loading input data\")\n",
    "# nifty_file = f\"{nifty_dir}images/017132_3.nii.gz\"\n",
    "nifty_file = \"/home/patrick/Workspace/cbs/SynthSeg/data/cbs/test_256_pd_r1_r2s/combined.nii\"\n",
    "img = nb.load(nifty_file)\n",
    "input_data = img.get_fdata()\n",
    "\n",
    "# Prepare the network\n",
    "print(\"Creating network\")\n",
    "net = M.unet(input_shape=(None, None, None, 3), n_labels=33)\n",
    "net.load_weights(net_file, by_name=True)\n",
    "\n",
    "current_shape = input_data.shape\n",
    "print(f\"Original shape {current_shape}\")\n",
    "\n",
    "# You want the size to be [512, 512, 512, 3]\n",
    "desired_shape = [256, 256, 256, 3]\n",
    "\n",
    "adjusted_data = adjust_size(input_data, desired_shape)\n",
    "print(\"Storing original but cropped image\")\n",
    "\n",
    "adjusted_data = np.flip(np.transpose(adjusted_data, [2, 0, 1, 3]), 1)/np.max(adjusted_data)\n",
    "epsilon = 1e-7  # define very close to 0 or 1\n",
    "outlier_mask = np.abs(adjusted_data) < epsilon\n",
    "adjusted_data[outlier_mask] = np.random.rand(np.count_nonzero(outlier_mask))/10.0  # replace with random values\n",
    "outlier_mask = np.abs(adjusted_data-1.0) < epsilon\n",
    "adjusted_data[outlier_mask] = np.random.rand(np.count_nonzero(outlier_mask))/10.0  # replace with random values\n",
    "\n",
    "# nb.save(nb.Nifti1Image(adjusted_data, np.eye(4), img.header), f\"{output_dir}image.nii\")\n",
    "# prediction = net.predict(np.expand_dims(adjusted_data, axis=0))\n",
    "\n",
    "print(\"Saving Segmentation\")\n",
    "# nb.save(nb.Nifti1Image(np.squeeze(softmax(prediction)), np.eye(4), img.header), f\"{output_dir}posteriors.nii\")\n",
    "# exit(0)\n",
    "\n",
    "# for number, order in enumerate(itertools.permutations([0, 1, 2])):\n",
    "#     print(f\"Predicting permutation {number}: {order}\")\n",
    "#     new_order = list(order) + [3]\n",
    "#     new_data = np.transpose(adjusted_data, new_order)\n",
    "#     nb.save(nb.Nifti1Image(new_data, np.eye(4), img.header), f\"{output_dir}image_{number}.nii\")\n",
    "#     # prediction = net.predict(np.expand_dims(new_data, axis=0))\n",
    "#     # nb.save(nb.Nifti1Image(np.squeeze(prediction), img.affine, img.header),\n",
    "#     #         f\"{output_dir}posteriors_{number}.nii\")\n",
    "\n",
    "\n",
    "# Code with 90 degree rotations and transpositions\n",
    "for number, order in enumerate(itertools.permutations([0, 1, 2])):\n",
    "    new_order = list(order) + [3]\n",
    "    rotated_data = adjusted_data\n",
    "    for axes in itertools.combinations([0, 1, 2], 2):  # All combinations of 2 axes in 3D\n",
    "        for k in range(2):  # 4 rotations for each combination of axes (0, 90, 180, 270 degrees)\n",
    "            print(f\"Predicting permutation {number} {axes} {k+1}\")\n",
    "            rotated_data = np.rot90(rotated_data, k+1, axes)\n",
    "            new_data = np.transpose(rotated_data, new_order)\n",
    "            nb.save(nb.Nifti1Image(new_data, np.eye(4), img.header), f\"{output_dir}image_{number}_{axes}_{k+1}.nii\")\n",
    "            prediction = net.predict(np.expand_dims(new_data, axis=0))\n",
    "            nb.save(nb.Nifti1Image(np.squeeze(softmax(prediction)), img.affine, img.header),\n",
    "                    f\"{output_dir}posteriors_{number}_{axes}_{k}.nii\")\n"
   ],
   "id": "4a52ebdba64eb127",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
