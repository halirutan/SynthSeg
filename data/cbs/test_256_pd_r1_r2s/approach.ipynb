{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Prerequisites\n",
    "\n",
    "Unless specified otherwise, we assume here that you have the virtual environment for the CBS fork of SynthSeg\n",
    "installed properly.\n",
    "To have direct access to the scripts used, please ensure you have installed this repository in the virtual environment by\n",
    "running the following in a terminal with activated environment from the project's folder:\n",
    "\n",
    "```shell\n",
    "pip install -e .\n",
    "```\n",
    "\n",
    "# Rundown of analysis, training, and prediction of multichannel segmentation\n",
    "\n",
    "The whole process consists of several steps that need to be repeated when training networks for different contrasts or resolutions.\n",
    "Here, we try to train a dedicated network for the AD/PCA highres data.\n",
    "The input for the segmentation will be\n",
    "\n",
    "- 500 um resolution scans of the whole head\n",
    "- we train on multichannel data that consists of PD, R1, and R2s maps\n",
    "- the plain input maps will be clipped and rescaled to suitable value ranges (because noisy regions contain large outliers that would reduce the range of meaningful contrast for the network to learn from)\n",
    "\n",
    "To run the whole pipeline, the following steps were necessary:\n",
    "\n",
    "1. Decide how many different contrasts you want as input for your network. Remember that for this approach the contrasts need to be perfectly co-registered. At the moment, I believe this makes the most sense when you're using multiple echoes or, like here, quantitative maps that show different features.\n",
    "2. Inspect the images and find suitable value ranges for all contrasts. These ranges are then used to clip and rescale the maps for further processing.\n",
    "3. For at least one set of input images, you need a segmentation. It doesn't need to be a perfect segmentation, but it will be used to calculate the regional gray value statistics for each segmentation class.\n",
    "4. From this analysis, a dedicated _generator configuration_ is created for you. It contains the specification for the random distribution for every segmentation class. This specification is used to create random synthetic maps that should resemble your original data as close as possible. In the case here, this will not be perfect because maps like R2s have gray value distributions in some regions that can not really be modeled with a Gaussian.\n",
    "5. The generator configuration is used to create the training data for the neural network. There are two options: You can create the training data on-the-fly, or you can create them beforehand and store them in a TensorFlow format that is easy to deserialize. The latter approach has another advantage. You can train different network parametrizations with the same training data and compare the performance.\n",
    "6. With the training data (or the generation configuration), you derive a training configuration that further specifies the used network model and its hyperparameters.\n",
    "7. During the training, we write out the weights and biases of the model as snapshots. These can be loaded back if you want to create a network for prediction (aka running the actual segmentation)\n"
   ],
   "id": "229a0007611ed0ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. & 2. Used Contrasts and Value Ranges\n",
    "\n",
    "The first of training a new network is to decide which contrasts to use and to clip and scale them to an appropriate range.\n",
    "SynthSeg (or networks in general) are happy with values [0, 1] or reasonable other ranges.\n",
    "However, if your images contain large outlier noise pixel, e.g. your main image information is in the range of [0, 1000] and there are\n",
    "outlier noise pixel with values > 20000, scaling might compress the important information too much.\n",
    "Also, the ranges in your image depend largely on the outlier noise and might hinder a good training and prediction.\n",
    "\n",
    "Therefore, the first step is to look into your maps/images and make a decision for good value ranges.\n",
    "In this experiment, we want to use three quantitative maps that are combined into a multichannel image and train the segmentation on it.\n",
    "We decided on the following maps, and we decided to use the following ranges:\n",
    "\n",
    "1. Proton Density (PD): Original gray value range: [-173181.7, 1933.817], clip to [0, 200], rescale to [0, 1]\n",
    "2. Relaxation Rate R1: Original gray value range: [0, 3.03], clip to [0, 3], rescale to [0, 1]\n",
    "3. R2s: Original gray value range: [-720.689, 1085.598], clip to [0, 200], rescale to [0, 1]\n",
    "\n",
    "The reason for clipping the maps is that both a \"density\" and the \"relaxation rates\" are non-negative by definition and have a much smaller\n",
    "range.\n",
    "The driver of appearing small and large outliers are the calculations of these values which do not work correctly on, e.g., noisy background."
   ],
   "id": "87709c1c6a5724eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. & 3. Running the Region Analysis and Create the Generator Config\n",
    "\n",
    "For analyzing the different regions of an example image, we need a segmentation.\n",
    "The goal is to have a rough (good enough) segmentation so that we can inspect each label and calculate statistics on the gray level distribution within\n",
    "each segmented region.\n",
    "Therefore, we take an image from our example and run it through standard SynthSeg and use this segmentation to analyze each labeled region.\n",
    "The only requirements are (a) you should get a half-decent segmentation and (b) the image you're using for the initial segmentation needs to be co-aligned\n",
    "(registered, in the same space) as the images you want to analyze.\n",
    "\n",
    "First, let's try a simple T1 map that we clip and rescale to have good values:"
   ],
   "id": "eb4f3d37c4679176"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from SynthSeg.analysis.contrast_analysis import clip_and_rescale_nifti\n",
    "\n",
    "input_dir = \"/Users/pscheibe/Documents/tmpData/SynthSeg/01\"\n",
    "output_dir = \"/Users/pscheibe/Documents/tmpData/SynthSeg/01/analysis\"\n",
    "\n",
    "initial_t1 = f\"{input_dir}/T1w_WOLSfit.nii\"\n",
    "t1_for_segmentation = f\"{output_dir}/T1w_WOLSfit_rescaled_for_segmentation.nii\"\n",
    "clip_and_rescale_nifti(\n",
    "    nifti_file=initial_t1,\n",
    "    out_file=t1_for_segmentation,\n",
    "    min_clip=0.0,\n",
    "    max_clip=2000,\n",
    "    min_out=0.0,\n",
    "    max_out=1.0\n",
    ")"
   ],
   "id": "3fe5b91bc0752f4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now, you use the default SynthSeg to get a 1mm segmentation. For that:\n",
    "\n",
    "1. Clone the original repo again (or switch and pull your existing SynthSeg repo)\n",
    "2. Download the necessary model snapshots that are linked in the [installation guide](https://github.com/BBillot/SynthSeg?tab=readme-ov-file#installation)\n",
    "3. Create the necessary Python environment and activate it\n",
    "\n",
    "Then you can segment the scaled T1w image from above.\n",
    "Check the command line arguments for SynthSeg.\n",
    "On my Mac here without GPU, I use:\n",
    "\n",
    "\n",
    "```shell\n",
    "python ./scripts/commands/SynthSeg_predict.py \\\n",
    "       --i $output_dir/T1w_WOLSfit.nii \\\n",
    "       --o $output_dir/T1w_WOLSfit_synthseg.nii \\\n",
    "       --robust --cpu --threads 8\n",
    "```\n",
    "\n",
    "### Running the Region Analysis\n",
    "\n",
    "The region analysis needs the following things:\n",
    "\n",
    "- A segmentation label image for the brain images you want to analyze\n",
    "- One or more contrast brain images that are going to be analyzed\n",
    "- Clipping and scaling parameters for each contrast image\n",
    "- (Optional) statistical settings that influence how each region is analyzed\n",
    "\n",
    "The region analysis creates the following output:\n",
    "\n",
    "- A template \"brain generator config\" named `_generator.yml` that defines the statistical values for the gray levels of each region\n",
    "- A clipped and scaled Nifty image for each input contrast image\n",
    "- A combined volume that has all clipped and scaled contrasts in different channels. This is basically similar to what the brain generator will create as training input.\n",
    "\n",
    "Using the created decent-enough-segmentation, the input contrasts and the values for clipping, you can call the analysis script as follows:\n",
    "\n",
    "```shell\n",
    "synthSeg-mpm-analyse \\\n",
    "  --label_file \"$output_dir/T1w_WOLSfit_synthseg.nii\" \\\n",
    "  --contrast_files \"$input_dir/PD.nii\" \"$input_dir/R1.nii\" \"$input_dir/R2s.nii\" \\\n",
    "  --output_dir \"$output_dir\" \\\n",
    "  --clip_min 0 0 0 --clip_max 200 2 200\n",
    "```\n",
    "\n",
    "Please also look at `synthSeg_mpm_analyse --help` to see all options.\n",
    "The contrast images and the combined image are scaled into the range [0,1] per default.\n",
    "When you prepare a training run, it is very helpful to provide a `template_generator_config` for the analysis call.\n",
    "You can fix all necessary paths and other settings because the analysis script will only touch options that handle the labels, channels and their gray value distributions."
   ],
   "id": "478f18e90a87b24f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 4. & 5. Creating the Training Data\n",
    "\n",
    "Let's start off by creating a `_generator.yml` directly in our experiment folder and set the gray value distribution parameters\n",
    "extremely narrow.\n",
    "Additionally, we use a config template where all augmentations are turned off.\n",
    "With this, we can generate test data and compare it with the maps that we analyzed.\n",
    "The gray levels should roughly follow what you find in the MPMs.\n",
    "\n",
    "```\n",
    "synthseg_data_dir=\"/Users/pscheibe/PycharmProjects/SynthSeg/data/cbs/test_256_pd_r1_r2s\" \n",
    "\n",
    "synthSeg-mpm-analyse \\\n",
    "  --label_file \"$output_dir/T1w_WOLSfit_synthseg.nii\" \\\n",
    "  --contrast_files \"$input_dir/PD.nii\" \\\n",
    "    \"$input_dir/R1.nii\" \\\n",
    "    \"$input_dir/R2s.nii\" \\\n",
    "  --output_dir \"$synthseg_data_dir\" \\\n",
    "  --clip_min 0 0 0 --clip_max 200 2 200 \\\n",
    "  --template_generator_config \"$synthseg_data_dir/analysis_template_no_augment.yml\" \\\n",
    "  --range_brackets 0.9999 1.0001 0.001 0.002\n",
    "```\n",
    "\n",
    "Next, we can create test data as follows:\n",
    "\n",
    "```\n",
    "test_data_dir=\"/Users/pscheibe/PycharmProjects/SynthSeg/output\"\n",
    "\n",
    "synthSeg-generate-synth-images \\\n",
    "  --config_file \"$synthseg_data_dir/_generator.yml\" \\\n",
    "  --output_dir \"$test_data_dir\" \\\n",
    "  --count 5\n",
    "```\n",
    "\n",
    "If this looks satisfactory, all augmentations like random rotations, non-linear transformations, bias-field, etc. need to be turned back on.\n",
    "Also, the `range_brackets` option should have reasonable values to give the generator enough freedom to create random contrasts while still following\n",
    "the distributions found in our maps.\n",
    "One important step: You might want to adjust the mean and std range for the first entry (background) in each channel.\n",
    "Roughly speaking, the MPM backgrounds contain many artifacts and by having a wide random range of gray values, the network hopefully learns to not use these regions for anything.\n",
    "\n",
    "\n",
    "\n",
    "## 6. Training Configuration and Training on HPC\n",
    "\n",
    "# Importing the necessary libraries and modules\n"
   ],
   "id": "7420cf8842830114"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T12:27:16.212765Z",
     "start_time": "2024-01-11T12:27:14.171100Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 13:27:14.206589: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 13:27:14.279029: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-11 13:27:15.595196: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 13:27:15.639732: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-11 13:27:15.639776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: amos\n",
      "2024-01-11 13:27:15.639787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: amos\n",
      "2024-01-11 13:27:15.639944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.147.5\n",
      "2024-01-11 13:27:15.639988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.147.5\n",
      "2024-01-11 13:27:15.639998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.147.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "import commands.cbs_predict as pred\n",
    "from SynthSeg.training_options import TrainingOptions\n",
    "from SynthSeg.brain_generator_options import GeneratorOptions\n",
    "\n",
    "\n",
    "training_config_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/training.yml\"\n",
    "generator_config_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/generator.yml\"\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "# Continue with the prediction part of your code.\n",
    "\n",
    "training_options = TrainingOptions.load_yaml(training_config_file)\n",
    "generator_options = GeneratorOptions.load_yaml(generator_config_file)\n",
    "\n",
    "training_options = training_options.convert_lists_to_numpy()\n",
    "generator_options = generator_options.convert_lists_to_numpy()\n",
    "\n",
    "predict_options = pred.PredictOptions(\n",
    "    training_config=training_config_file,\n",
    "    path_images=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/tfrecord2nifty/images\",\n",
    "    gt_folder=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/tfrecord2nifty/labels\",\n",
    "    path_model=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/dice_063.h5\",\n",
    "    path_segmentations=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/segmentations/\",\n",
    "    path_posteriors=\"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/posteriors/\",\n",
    "    target_res=training_options.target_res,\n",
    ")"
   ],
   "id": "6911d9f6c7020e53",
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T12:50:20.016090Z",
     "start_time": "2024-01-11T12:37:02.322328Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting 1/8\n",
      "1/1 [==============================] - 17s 17s/step\n",
      "predicting 2/8    remaining time: 0:11:09\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 3/8    remaining time: 0:09:16\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "predicting 4/8    remaining time: 0:07:45\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "predicting 5/8    remaining time: 0:06:11\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 6/8    remaining time: 0:04:41\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 7/8    remaining time: 0:03:07\n",
      "1/1 [==============================] - 15s 15s/step\n",
      "predicting 8/8    remaining time: 0:01:33\n",
      "1/1 [==============================] - 14s 14s/step\n",
      "evaluating 1/8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred.predict(\n",
    "    predict_options.path_images,\n",
    "    predict_options.path_segmentations,\n",
    "    predict_options.path_model,\n",
    "    generator_options.output_labels,\n",
    "    n_neutral_labels=generator_options.n_neutral_labels,\n",
    "    names_segmentation=predict_options.names_segmentation,\n",
    "    path_posteriors=predict_options.path_posteriors,\n",
    "    path_resampled=predict_options.path_resampled,\n",
    "    path_volumes=predict_options.path_volumes,\n",
    "    min_pad=predict_options.min_pad,\n",
    "    cropping=predict_options.cropping,\n",
    "    target_res=predict_options.target_res,\n",
    "    gradients=predict_options.gradients,\n",
    "    flip=predict_options.flip,\n",
    "    topology_classes=predict_options.topology_classes,\n",
    "    sigma_smoothing=predict_options.sigma_smoothing,\n",
    "    keep_biggest_component=predict_options.keep_biggest_component,\n",
    "    n_levels=training_options.n_levels,\n",
    "    nb_conv_per_level=training_options.nb_conv_per_level,\n",
    "    conv_size=training_options.conv_size,\n",
    "    unet_feat_count=training_options.unet_feat_count,\n",
    "    feat_multiplier=training_options.feat_multiplier,\n",
    "    activation=training_options.activation,\n",
    "    gt_folder=predict_options.gt_folder,\n",
    "    evaluation_labels=predict_options.evaluation_labels,\n",
    "    list_incorrect_labels=predict_options.list_incorrect_labels,\n",
    "    list_correct_labels=predict_options.list_correct_labels,\n",
    "    compute_distances=predict_options.compute_distances,\n",
    "    recompute=predict_options.recompute,\n",
    "    verbose=predict_options.verbose,\n",
    "    use_original_unet=training_options.use_original_unet\n",
    ")\n"
   ],
   "id": "e9ec5c56cf270cec",
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-11T03:56:08.083859Z",
     "start_time": "2024-01-11T03:56:08.078178Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,  14,  15,  16,  24,  72,  85, 502, 506, 507, 508, 509, 511,\n",
       "       512, 514, 515, 516, 530,   2,   3,   4,   5,   7,   8,  10,  11,\n",
       "        12,  13,  17,  18,  25,  26,  28,  30, 136, 137,  41,  42,  43,\n",
       "        44,  46,  47,  49,  50,  51,  52,  53,  54,  57,  58,  60,  62,\n",
       "       163, 164])"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 3
    }
   ],
   "source": [
    "generator_options.generation_labels"
   ],
   "id": "771294538c272ade",
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nibabel as nb\n",
    "import SynthSeg.segmentation_model as M\n",
    "import commands.tfrecord_to_nifti as tfnii\n",
    "import itertools\n",
    "from typing import List\n",
    "\n",
    "# Uncomment this if you run out of GPU memory when predicting\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "print(\"Using GPU\" if tf.test.gpu_device_name() else \"Using CPU\")\n",
    "\n",
    "tfrecord_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/017132.tfrecord\"\n",
    "nifty_dir = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/tfrecord2nifty/\"\n",
    "net_file = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/dice_063.h5\"\n",
    "output_dir = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/cbs_data/\"\n",
    "# output_dir = \"/home/patrick/Documents/CBS/DataShare/SynthSeg/Models/202401/predictions/training_data/\"\n",
    "\n",
    "\n",
    "def adjust_size(input_data: np.ndarray, desired_shape: List[int]):\n",
    "    \"\"\"\n",
    "    Adjusts the size of the input data array to match the desired shape.\n",
    "\n",
    "    It does it by either cropping or padding symmetrically.\n",
    "\n",
    "    Args:\n",
    "        input_data: The input data array.\n",
    "        desired_shape: The desired shape for the input data array.\n",
    "\n",
    "    Returns:\n",
    "        The input data array adjusted to the desired shape.\n",
    "    \"\"\"\n",
    "    current_shape = input_data.shape\n",
    "    adjustments = [(ds - cs) / 2. for ds, cs in zip(desired_shape, current_shape)]\n",
    "    pad_values = [(int(np.floor(adj)), int(np.ceil(adj))) if adj > 0 else (0, 0) for adj in adjustments]\n",
    "    crop_values = [(int(np.floor(-adj)), -int(np.ceil(-adj))) if adj < 0 else (0, 0) for adj in adjustments]\n",
    "    content_values = [(int(np.floor(adj)), int(cs + np.ceil(adj))) for adj, cs in zip(adjustments, current_shape)]\n",
    "    # Pad / Crop the array for non-negative / negative adjustments (respectively)\n",
    "    input_data_adjusted = np.pad(input_data, pad_width=pad_values, mode='constant', constant_values=0)\n",
    "    input_data_adjusted = input_data_adjusted[\n",
    "                          crop_values[0][0]:content_values[0][1],\n",
    "                          crop_values[1][0]:content_values[1][1],\n",
    "                          crop_values[2][0]:content_values[2][1]]\n",
    "\n",
    "    return input_data_adjusted\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "# Create nifti from a tfrecord file.\n",
    "# tfnii.tfrecord_to_nifty(tfnii.Options(input_file=tfrecord_file, output_directory=nifty_dir))\n",
    "\n",
    "print(\"Loading input data\")\n",
    "# nifty_file = f\"{nifty_dir}images/017132_3.nii.gz\"\n",
    "nifty_file = \"/home/patrick/Workspace/cbs/SynthSeg/data/cbs/test_256_pd_r1_r2s/combined.nii\"\n",
    "img = nb.load(nifty_file)\n",
    "input_data = img.get_fdata()\n",
    "\n",
    "# Prepare the network\n",
    "print(\"Creating network\")\n",
    "net = M.unet(input_shape=(None, None, None, 3), n_labels=33)\n",
    "net.load_weights(net_file, by_name=True)\n",
    "\n",
    "current_shape = input_data.shape\n",
    "print(f\"Original shape {current_shape}\")\n",
    "\n",
    "# You want the size to be [512, 512, 512, 3]\n",
    "desired_shape = [256, 256, 256, 3]\n",
    "\n",
    "adjusted_data = adjust_size(input_data, desired_shape)\n",
    "print(\"Storing original but cropped image\")\n",
    "\n",
    "adjusted_data = np.flip(np.transpose(adjusted_data, [2, 0, 1, 3]), 1)/np.max(adjusted_data)\n",
    "epsilon = 1e-7  # define very close to 0 or 1\n",
    "outlier_mask = np.abs(adjusted_data) < epsilon\n",
    "adjusted_data[outlier_mask] = np.random.rand(np.count_nonzero(outlier_mask))/10.0  # replace with random values\n",
    "outlier_mask = np.abs(adjusted_data-1.0) < epsilon\n",
    "adjusted_data[outlier_mask] = np.random.rand(np.count_nonzero(outlier_mask))/10.0  # replace with random values\n",
    "\n",
    "# nb.save(nb.Nifti1Image(adjusted_data, np.eye(4), img.header), f\"{output_dir}image.nii\")\n",
    "# prediction = net.predict(np.expand_dims(adjusted_data, axis=0))\n",
    "\n",
    "print(\"Saving Segmentation\")\n",
    "# nb.save(nb.Nifti1Image(np.squeeze(softmax(prediction)), np.eye(4), img.header), f\"{output_dir}posteriors.nii\")\n",
    "# exit(0)\n",
    "\n",
    "# for number, order in enumerate(itertools.permutations([0, 1, 2])):\n",
    "#     print(f\"Predicting permutation {number}: {order}\")\n",
    "#     new_order = list(order) + [3]\n",
    "#     new_data = np.transpose(adjusted_data, new_order)\n",
    "#     nb.save(nb.Nifti1Image(new_data, np.eye(4), img.header), f\"{output_dir}image_{number}.nii\")\n",
    "#     # prediction = net.predict(np.expand_dims(new_data, axis=0))\n",
    "#     # nb.save(nb.Nifti1Image(np.squeeze(prediction), img.affine, img.header),\n",
    "#     #         f\"{output_dir}posteriors_{number}.nii\")\n",
    "\n",
    "\n",
    "# Code with 90 degree rotations and transpositions\n",
    "for number, order in enumerate(itertools.permutations([0, 1, 2])):\n",
    "    new_order = list(order) + [3]\n",
    "    rotated_data = adjusted_data\n",
    "    for axes in itertools.combinations([0, 1, 2], 2):  # All combinations of 2 axes in 3D\n",
    "        for k in range(2):  # 4 rotations for each combination of axes (0, 90, 180, 270 degrees)\n",
    "            print(f\"Predicting permutation {number} {axes} {k+1}\")\n",
    "            rotated_data = np.rot90(rotated_data, k+1, axes)\n",
    "            new_data = np.transpose(rotated_data, new_order)\n",
    "            nb.save(nb.Nifti1Image(new_data, np.eye(4), img.header), f\"{output_dir}image_{number}_{axes}_{k+1}.nii\")\n",
    "            prediction = net.predict(np.expand_dims(new_data, axis=0))\n",
    "            nb.save(nb.Nifti1Image(np.squeeze(softmax(prediction)), img.affine, img.header),\n",
    "                    f\"{output_dir}posteriors_{number}_{axes}_{k}.nii\")\n"
   ],
   "id": "4a52ebdba64eb127",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
