{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Rundown of analysis, training, and prediction of multichannel segmentation\n",
    "\n",
    "The whole process consists of several steps that need to be repeated when training networks for different contrasts or resolutions.\n",
    "Here, we try to train a dedicated network for the AD/PCA highres data.\n",
    "The input for the segmentation will be\n",
    "\n",
    "- 500 um resolution scans of the whole head\n",
    "- we train on multichannel data that consists of PD, R1, and R2s maps\n",
    "- the plain input maps will be clipped and rescaled to suitable value ranges (because noisy regions contain large outliers that would reduce the range of meaningful contrast for the network to learn from)\n",
    "\n",
    "To run the whole pipeline, the following steps were necessary:\n",
    "\n",
    "1. Decide how many different contrasts you want as input for your network. Remember that for this approach the contrasts need to be perfectly co-registered. At the moment, I believe this makes the most sense when you're using multiple echoes or, like here, quantitative maps that show different features.\n",
    "2. Inspect the images and find suitable value ranges for all contrasts. These ranges are then used to clip and rescale the maps for further processing.\n",
    "3. For at least one set of input images, you need a segmentation. It doesn't need to be a perfect segmentation, but it will be used to calculate the regional gray value statistics for each segmentation class.\n",
    "4. From this analysis, a dedicated _generator configuration_ is created for you. It contains the specification for the random distribution for every segmentation class. This specification is used to create random synthetic maps that should resemble your original data as close as possible. In the case here, this will not be perfect because maps like R2s have gray value distributions in some regions that can not really be modeled with a Gaussian.\n",
    "5. The generator configuration is used to create the training data for the neural network. There are two options: You can create the training data on-the-fly, or you can create them beforehand and store them in a TensorFlow format that is easy to deserialize. The latter approach has another advantage. You can train different network parametrizations with the same training data and compare the performance.\n",
    "6. With the training data (or the generation configuration), you derive a training configuration that further specifies the used network model and its hyperparameters.\n",
    "7. During the training, we write out the weights and biases of the model as snapshots. These can be loaded back if you want to create a network for prediction (aka running the actual segmentation)\n",
    "\n",
    "## 1. & 2. Used Contrasts and Value Ranges\n",
    "\n",
    "## 2. & 3. Running the Region Analysis and Create the Generator Config\n",
    "\n",
    "## 4. & 5. Creating the Training Data\n",
    "\n",
    "## 6. Training Configuration and Training on HPC\n",
    "\n",
    "## 7. Prediction\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab4ee892f044f32f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
